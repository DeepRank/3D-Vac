from matplotlib import pyplot as plt
import torch.nn.functional as F
import torch.nn as nn
import torch
import h5py
import os
import argparse
import numpy as np
from copy import deepcopy

import sys
sys.path.append('/home/dmarz/3D-Vac/src/5_train_models')
from DeepRank import classMetrics

arg_parser = argparse.ArgumentParser(
    description = ""
)
arg_parser.add_argument("--metricsfile", "-f",
    help="path of HDF5 file containing metrics per epoch generated by deeprank.learn.NeuralNet",
    type=str,
    required=True
)
arg_parser.add_argument("--overwrite-plots", "-o",
    help="overwrite plots in output folder if already present",
    action='store_true',
    default= False,
)

def check_path_exists(pathname):
    if os.path.exists(pathname):
        return True
    
def export_losses(losses_dict, n_epochs, outdir, best_model=False, show=False, overwrite=False, title=False):
    """Plot the losses vs the epoch.
    
    Args:
        figname (str): name of the file where to export the figure
    """
    
    color_plot = ['red', 'blue', 'green']
    labels = ['train', 'valid', 'test']
    
    # if best_model:
    #     best_model = ((np.where(np.array(losses_dict['valid']) == self.min_error['valid'])[0][-1] \
    #                     +1)*self.frac_measure)-1
    
    fig, ax = plt.subplots()
    for ik, name in enumerate(losses_dict):
        # if name == 'test' or name == 'train':
        #     x = np.linspace(1, n_epochs, len(losses_dict[name]))
        # else:
        #     x = np.linspace(1, n_epochs, len(losses_dict[name]))
        if name == 'train':
            x = np.linspace(1, n_epochs, len(losses_dict[name]))
        else:
            x = np.linspace(0, n_epochs, len(losses_dict[name]))
            
        plt.plot(   x,
                    np.array(losses_dict[name]),
                    c = color_plot[ik],
                    label = labels[ik])
        plt.scatter(   x,
                    np.array(losses_dict[name]),
                    c = color_plot[ik],
                    s=8, marker='D',
                    label='_nolegend_')

    if best_model:
        plt.axvline(x = best_model+1, #adjustment due to epoch -001 
                    color = 'orchid', 
                    ls='--', 
                    label = 'best model')
        
    legend = ax.legend(loc='upper right')
    
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Losses')
    ax.set_xticks(np.arange(0, len(x)+1, 1))
    if title:
        plt.title(f'{title} loss')
    
    if show:
        plt.show()
        # outdir= '/home/severin/teststuff/deeprank_plot'
        figname = os.path.join(outdir, 'losses.png')
        if not check_path_exists(figname) or overwrite:
            print(f'writing losses plot to {figname}')
            plt.savefig(figname, dpi=300)
        elif check_path_exists(figname):
            print(f'path {figname} exists, give --overwrite as argument if file needs to be overwritten')
        
    plt.close()
    
def export_metrics(metricname:str, classmetrics:dict, exec_epochs:int, outdir:str=None, 
                   best_model=False, show=False, overwrite=False, title=False):
    """seperate plot for each metric (e.g accuracy/auc) over the epochs
    
    Args:
        metricname (str): name of the metric to obtain from classmetrics dict and to plot
        exec_epochs (int): number of executed epochs (may have been corrected for early stopping)
    """
    
    # if best_model:
    #     best_model = ((np.where(np.array(f_metrics['losses/valid'][:]) == self.min_error['valid'])[0][-1]+1)\
    #     *1)-1        
    
    color_plot = ['red', 'blue', 'green']
    labels = ['train', 'valid', 'test']
    
    data = classmetrics[metricname]
    fig, ax = plt.subplots()
    for ik, name in enumerate(data):
        
        if name == 'train':
            x = np.linspace(1, exec_epochs, len(data[name]))
        else:
            x = np.linspace(0, exec_epochs, len(data[name]))
        plt.plot(x, np.array(data[name]), c=color_plot[ik], label=labels[ik])
        
    if best_model:
        plt.axvline(x = best_model+1, #adjustment due to epoch -001 
                    color = 'orchid', 
                    ls='--', 
                    label = 'best model')
    
    legend = ax.legend(loc='upper left')
    ax.set_xlabel('Epoch')
    ax.set_ylabel(metricname.upper())
    ax.set_xticks(np.arange(0, len(x)+1, 1))
    if title:
        plt.title(f'{title} {metricname}')
    
    if show:
        print(f'\n --> {metricname.upper()} Plot')
        plt.show()
    
        # outdir= '/home/severin/teststuff/deeprank_plot'
        figname = os.path.join(outdir, metricname + '.png')
        if not check_path_exists(figname) or overwrite:
            print(f'writing metrics ({metricname}) plot to {figname}')
            plt.savefig(figname, dpi=300)
        elif check_path_exists(figname):
            print(f'path {figname} exists, give --overwrite as argument if file needs to be overwritten')
        
    plt.close()
    if best_model:
        return np.array(data[name])[best_model]
    
def get_losses_per_epoch(f_metrics, f_metrics_keys):

    if np.array(f_metrics[f'epoch_0001/train/outputs']).shape[1] == 2:
        criterion = nn.CrossEntropyLoss(weight = None, reduction='mean')
    elif np.array(f_metrics[f'epoch_0001/train/outputs']).shape[1] == 1:
        criterion = nn.MSELoss(reduction='mean')
    losses_dict = {'train': [], 'valid': [], 'test': []}
    for epoch in f_metrics_keys:
        for subset in f_metrics[epoch]:
            outputs = np.array(f_metrics[f'{epoch}/{subset}/outputs'])
            targets = np.array(f_metrics[f'{epoch}/{subset}/targets'])
            
            loss = criterion(torch.Tensor(outputs), torch.Tensor(targets).long())
            
            if subset == 'test':
                losses_dict['test'].append(loss)
            elif subset == 'valid':
                losses_dict['valid'].append(loss)
            else:
                losses_dict['train'].append(loss)
    return losses_dict

def get_metrics_per_epoch(f_metrics, f_metrics_keys, metrics):
    metrics_dir = {}
    for m in metrics:
        metrics_dir[m] = {'train': [], 'valid': [], 'test': []}
        
    for i, epoch in enumerate(f_metrics_keys):
        if i == 0:
            for m in metrics:
                try:
                    metrics_dir[m]['valid'].append(f_metrics[f'{epoch}/valid/{m}'][()])
                    metrics_dir[m]['test'].append(f_metrics[f'{epoch}/test/{m}'][()])
                except KeyError:
                    f_metrics.close()
                    raise Exception(f'exception at line 159 caused by m {m} and epoch {epoch}')
            
        else:
            for m in metrics:
                metrics_dir[m]['train'].append(f_metrics[f'{epoch}/train/{m}'][()])
                metrics_dir[m]['valid'].append(f_metrics[f'{epoch}/valid/{m}'][()])
                metrics_dir[m]['test'].append(f_metrics[f'{epoch}/test/{m}'][()])
    return metrics_dir

def calculate_metrics_per_epoch(outputs_dict, f_metrics_keys, metrics):
    metrics_dir = {}
    for m in metrics:
        metrics_dir[m] = {'train': [], 'valid': [], 'test': []}
        
    for i, epoch in enumerate(outputs_dict['valid']['outputs']):
        if i == 0:
            for m in metrics:
                try:
                    metrics_dir[m]['valid'].append(calculate_metric(m,outputs_dict['valid']['outputs'][i][:], outputs_dict['valid']['targets'][i][:]))
                    metrics_dir[m]['test'].append(calculate_metric(m,outputs_dict['test']['outputs'][i][:], outputs_dict['test']['targets'][i][:]))
                except KeyError:
                    raise Exception(f'exception caused by m {m} and epoch {epoch}')
            
        else:
            for m in metrics:
                metrics_dir[m]['train'].append(calculate_metric(m,outputs_dict['train']['outputs'][i-1][:], outputs_dict['train']['targets'][i-1][:]))
                metrics_dir[m]['valid'].append(calculate_metric(m,outputs_dict['valid']['outputs'][i][:], outputs_dict['valid']['targets'][i][:]))
                metrics_dir[m]['test'].append(calculate_metric(m,outputs_dict['test']['outputs'][i][:], outputs_dict['test']['targets'][i][:]))
    return metrics_dir


def get_outputs_per_epoch(f_metrics, f_metrics_keys, metrics):
    outputs_dict = {'train': {'outputs': [], 'targets': [], 'continuous_outputs': [],'continuous_targets': []}, 
                    'valid': {'outputs': [], 'targets': [], 'continuous_outputs': [],'continuous_targets': []}, 
                    'test': {'outputs': [], 'targets': [], 'continuous_outputs': [],'continuous_targets': []}}
    for i, epoch in enumerate(f_metrics_keys):
        #print(i)
        for subset in f_metrics[epoch]:
            outputs = np.array(f_metrics[f'{epoch}/{subset}/outputs'])
            targets = np.array(f_metrics[f'{epoch}/{subset}/targets'])
            continuous_targets = deepcopy(targets)
            continuous_outputs = deepcopy(outputs)

            output_2dim = np.zeros((outputs.shape[0], 2))
            output_2dim[:,0] = 1-outputs[:,0]
            output_2dim[:,1] = outputs[:,0]
            outputs = np.array(torch.Tensor(output_2dim))
            
            # also convert targets to binary
            class_label_cutoff = 1-(np.log10(max(min(500, 50_000), 1))/np.log10(50_000))
            targets = (targets > class_label_cutoff).astype('int')
            try:
                targets = targets[:,0]
            except IndexError:
                pass
            outputs_dict[subset]['outputs'].append(get_binclass_prediction(outputs))
            outputs_dict[subset]['targets'].append(targets)
            outputs_dict[subset]['continuous_outputs'].append(continuous_outputs)
            outputs_dict[subset]['continuous_targets'].append(continuous_targets)
            
    return outputs_dict

def get_binclass_prediction(outputs):

    probility = F.softmax(torch.FloatTensor(outputs), dim=1).data.numpy()
    pred = probility[:, 0] <= probility[:, 1]
    return pred.astype(int)

def calculate_metric(metricname, pred, targets):
    if metricname == 'acc':
        return classMetrics.accuracy(pred, targets)
        # acc = classMetrics.accuracy(pred, targets)
        # raise Exception((acc, pred, targets))
    elif metricname == 'tpr':
        return classMetrics.sensitivity(pred, targets)
    elif metricname == 'tnr':
        return classMetrics.specificity(pred, targets)
    elif metricname == 'ppv':
        return classMetrics.precision(pred, targets)
    elif metricname == 'f1':
        return classMetrics.F1(pred, targets)
    elif metricname == 'mcc':
        return classMetrics.mcc(pred, targets)
    elif metricname == 'auc':
        #pred = data['outputs'][:,1] if self.task == 'class' else data['outputs']
        try:
            #pred = np.array(pred)[:,0]
            #targets = [x[0] for x in targets]
            roc = classMetrics.roc_auc(pred, targets)
        except Exception as e:
            raise Exception(('PRED:', pred, 'TARGETS:', targets))
        return roc
    elif metricname == 'rmse':
        #return classMetrics.rmse(pred, targets)
        pass


if __name__ == "__main__":
    
    metrics = ['acc', 'auc', 'f1', 'mcc', 'tnr', 'tpr']
    
    args = arg_parser.parse_args()
    
    overwrite = args.overwrite_plots
    
    training_path = os.path.dirname(args.metricsfile)
    
    f_metrics = h5py.File(args.metricsfile, 'r')
    
    if list(f_metrics.keys())[-1] == 'losses':
        f_metrics_keys = list(f_metrics.keys())[:-1]
    else:
        f_metrics_keys = list(f_metrics.keys())
        
    metrics_dict = get_metrics_per_epoch(f_metrics, f_metrics_keys, metrics)
    losses_dict = get_losses_per_epoch(f_metrics, f_metrics_keys)
    
    f_metrics.close()
    
    export_losses(losses_dict, len(f_metrics_keys), training_path)
    
    best_model = (np.where(np.array(
        f_metrics['losses/valid'][:]) == min(f_metrics['losses/valid'][:]))[0][-1])
    
    for m in metrics:
        export_metrics(m, metrics_dict, len(f_metrics_keys), training_path, best_model=best_model)

