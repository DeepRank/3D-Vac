{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USAGE\n",
    "- Change the path to models generated by `cnn_baseline.py` (if needed).\n",
    "- Change the `model_name` with the output name of `cnn_baseline.py` script, which is also the folder name containing 10 fold cross validation data.\n",
    "- Run all code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daqop/anaconda3/envs/deeprank/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from torch.nn.functional import cross_entropy\n",
    "from math import sqrt\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(metrics_f):\n",
    "    # Load the metrics file:\n",
    "    h5 = h5py.File(metrics_f)\n",
    "    h5_keys = list(h5.keys())\n",
    "\n",
    "    # Retrieve test metrics from the best model (5/CNN/I/classification/struct/cnn_performances.py output)\n",
    "    # associated to the metrics data:\n",
    "    model_name = re.search(r\"(?<=/trained_models/)[a-z0-9_]*(?=/)\", metrics_f, re.IGNORECASE).group(0) # folder name extracted from the path\n",
    "    # using regular expression\n",
    "    fold = re.search(r\"(?<=/)[0-9](?=/)\", metrics_f).group(0) # current fold (from 0 to 10)\n",
    "    test_data_f = f\"/home/daqop/Desktop/M2/Internship/python/tested_models/{model_name}/{fold}/test_data.hdf5\" # retrieve the \n",
    "    # associated performances of the best model of this fold\n",
    "    test_h5 = h5py.File(test_data_f)\n",
    "    \n",
    "    logit = torch.tensor(test_h5[\"epoch_0000/test/outputs\"]) # row probability\n",
    "    pred = logit.max(1)[1] # rounded probability\n",
    "    targets = torch.tensor(test_h5[\"epoch_0000/test/targets\"])\n",
    "\n",
    "    confusion = pred/targets # absolute values for metrics\n",
    "    tot = targets.shape[0] # total number of prediction\n",
    "    pos = float((targets == 1.).sum()) # total number of positives (truth 1)\n",
    "    neg = float((targets == 0.).sum()) # total number of negatives (truth 0)\n",
    "\n",
    "    best_model_test_tpr = float((confusion == 1.).sum()/pos) # true prositive rate = prediction 1/truth 1\n",
    "    best_model_test_tnr = float((torch.isnan(confusion)).sum()/neg) # true prositive rate = prediction 1/truth 1\n",
    "    best_model_test_accuracy = float((pred==targets).sum()/tot)\n",
    "    roc_tpr, roc_fpr, _ = roc_curve(targets, logit.max(1)[0], pos_label=1.)\n",
    "    best_model_test_auc = auc(roc_fpr, roc_tpr)\n",
    "    best_model_test_mcc = matthews_corrcoef(targets, pred)\n",
    "\n",
    "    train_tpr = []\n",
    "    train_losses = []\n",
    "    train_tnr = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    valid_tpr = []\n",
    "    valid_losses = []\n",
    "    valid_tnr = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    test_tpr = []\n",
    "    test_losses = []\n",
    "    test_tnr = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for key in h5_keys:\n",
    "        if \"epoch_\" in key:\n",
    "            train_outputs = torch.tensor(h5[key][\"train\"][\"outputs\"], dtype=torch.float)\n",
    "            valid_outputs = torch.tensor(h5[key][\"valid\"][\"outputs\"], dtype=torch.float)\n",
    "            test_outputs = torch.tensor(h5[key][\"test\"][\"outputs\"], dtype=torch.float)\n",
    "\n",
    "            train_labels = torch.tensor(h5[key][\"train\"][\"targets\"])\n",
    "            valid_labels = torch.tensor(h5[key][\"valid\"][\"targets\"])\n",
    "            test_labels = torch.tensor(h5[key][\"test\"][\"targets\"])\n",
    "\n",
    "            \n",
    "            train_tpr.append(h5[key][\"train\"][\"tpr\"][()])\n",
    "            train_tnr.append(h5[key][\"train\"][\"tnr\"][()])\n",
    "            train_accuracies.append(h5[key][\"train\"][\"acc\"][()])\n",
    "            train_losses.append(cross_entropy(train_outputs, train_labels))\n",
    "\n",
    "            valid_tpr.append(h5[key][\"valid\"][\"tpr\"][()])\n",
    "            valid_tnr.append(h5[key][\"valid\"][\"tnr\"][()])\n",
    "            valid_accuracies.append(h5[key][\"valid\"][\"acc\"][()])\n",
    "            valid_losses.append(cross_entropy(valid_outputs, valid_labels))\n",
    "\n",
    "            test_tpr.append(h5[key][\"test\"][\"tpr\"][()])\n",
    "            test_tnr.append(h5[key][\"test\"][\"tnr\"][()])\n",
    "            test_accuracies.append(h5[key][\"test\"][\"acc\"][()])\n",
    "            test_losses.append(cross_entropy(test_outputs, test_labels))\n",
    "\n",
    "    return \\\n",
    "        best_model_test_tpr, best_model_test_tnr, \\\n",
    "        best_model_test_auc, best_model_test_mcc, best_model_test_accuracy,\\\n",
    "        train_accuracies, valid_accuracies, test_accuracies,\\\n",
    "        train_tpr, valid_tpr, test_tpr,\\\n",
    "        train_tnr, valid_tnr, test_tnr,\\\n",
    "        train_losses, valid_losses, test_losses\n",
    "\n",
    "def save_best_metrics(model_name, test_best_tpr, test_best_tnr, test_auc, test_mcc,\n",
    "    test_best_accuracies\n",
    "):\n",
    "    d = {\n",
    "        \"sensitivity_mean\": [float(test_best_tpr.mean())],\n",
    "        \"sensitivity_std\": [float(test_best_tpr.std())],\n",
    "        \"specificity_mean\": [float(test_best_tnr.mean())],\n",
    "        \"specificity_std\": [float(test_best_tnr.std())],\n",
    "        \"auc_mean\": [float(test_auc.mean())],\n",
    "        \"auc_std\": [float(test_auc.std())],\n",
    "        \"mcc_mean\": [float(test_mcc.mean())],\n",
    "        \"mcc_std\": [float(test_mcc.std())],\n",
    "        \"accuracy_mean\": [float(test_best_accuracies.mean())],\n",
    "        \"accuracy_std\": [float(test_best_accuracies.std())],\n",
    "        \"model_path\": [model_name],\n",
    "    }\n",
    "    df = pd.DataFrame(d)\n",
    "    metrics_csv_path = \"./best_models_metrics.csv\"\n",
    "    if os.path.exists(metrics_csv_path):\n",
    "        csv_df = pd.read_csv(metrics_csv_path)\n",
    "        concat_df = pd.concat([csv_df, df])\n",
    "        concat_df.to_csv(metrics_csv_path, index=False)\n",
    "    else:\n",
    "        df.to_csv(metrics_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17472/31912223.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  logit = torch.tensor(test_h5[\"epoch_0000/test/outputs\"]) # row probability\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cnn_3_conv_layers\" # This line is the input for the model name\n",
    "models_path = \"/home/daqop/Desktop/M2/Internship/python/trained_models\" # Path where all models are located\n",
    "metrics_files = glob.glob(f\"{models_path}/{model_name}/*/metrics.hdf5\")\n",
    "\n",
    "all_best_model_train_tpr, all_best_model_train_tnr, \\\n",
    "best_model_test_aucs, best_model_test_mccs, best_model_test_accuracies,\\\n",
    "all_train_accuracies, all_valid_accuracies, all_test_accuracies, \\\n",
    "all_train_tpr, all_valid_tpr, all_test_tpr, \\\n",
    "all_train_tnr, all_valid_tnr, all_test_tnr, \\\n",
    "all_train_losses, all_valid_losses, all_test_losses = [],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "for metrics_file in metrics_files:\n",
    "    best_model_train_tpr, best_model_train_tnr, \\\n",
    "    best_model_test_auc, best_model_test_mcc, best_model_test_accuracy, \\\n",
    "    train_accuracy, valid_accuracy, test_accuracy, \\\n",
    "    train_tpr, valid_tpr, test_tpr, \\\n",
    "    train_tnr, valid_tnr, test_tnr, \\\n",
    "    train_losses, valid_losses, test_losses = get_metrics(metrics_file)\n",
    "\n",
    "    all_best_model_train_tpr.append(best_model_train_tpr);all_best_model_train_tnr.append(best_model_train_tnr);\n",
    "\n",
    "    best_model_test_aucs.append(best_model_test_auc);best_model_test_mccs.append(best_model_test_mcc)\n",
    "    best_model_test_accuracies.append(best_model_test_accuracy)\n",
    "    \n",
    "    all_train_accuracies.append(train_accuracy);all_valid_accuracies.append(valid_accuracy);all_test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    all_train_tpr.append(train_tpr);all_valid_tpr.append(valid_tpr);all_test_tpr.append(test_tpr)\n",
    "\n",
    "    all_train_tnr.append(train_tnr);all_valid_tnr.append(valid_tnr);all_test_tnr.append(test_tnr)\n",
    "\n",
    "    all_train_losses.append(train_losses);all_valid_losses.append(valid_losses);all_test_losses.append(test_losses)\n",
    "\n",
    "test_auc = torch.tensor(best_model_test_aucs)\n",
    "test_mcc = torch.tensor(best_model_test_mccs)\n",
    "test_best_tpr = torch.tensor(all_best_model_train_tpr)\n",
    "test_best_tnr = torch.tensor(all_best_model_train_tnr)\n",
    "test_best_accuracies = torch.tensor(best_model_test_accuracies)\n",
    "\n",
    "train_accuracies = torch.permute(torch.tensor(all_train_accuracies), (1,0))\n",
    "valid_accuracies = torch.permute(torch.tensor(all_valid_accuracies), (1,0))\n",
    "test_accuracies = torch.permute(torch.tensor(all_test_accuracies), (1,0))\n",
    "\n",
    "train_tpr = torch.permute(torch.tensor(all_train_tpr), (1,0))\n",
    "valid_tpr = torch.permute(torch.tensor(all_valid_tpr), (1,0))\n",
    "test_tpr = torch.permute(torch.tensor(all_test_tpr), (1,0))\n",
    "\n",
    "train_tnr = torch.permute(torch.tensor(all_train_tnr), (1,0))\n",
    "valid_tnr = torch.permute(torch.tensor(all_valid_tnr), (1,0))\n",
    "test_tnr = torch.permute(torch.tensor(all_test_tnr), (1,0))\n",
    "\n",
    "train_losses = torch.permute(torch.tensor(all_train_losses), (1,0))\n",
    "valid_losses = torch.permute(torch.tensor(all_valid_losses), (1,0))\n",
    "test_losses = torch.permute(torch.tensor(all_test_losses), (1,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_metrics(model_name, test_best_tpr, test_best_tnr, test_auc, test_mcc,\n",
    "    test_best_accuracies    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "((ax1, ax2), (ax3, ax4)) = fig.subplots(2, 2)\n",
    "fig.suptitle(f\"CNN performances on clustered data\", color=\"red\", fontsize=18)\n",
    "fig.text(\n",
    "    0.15,1,\n",
    "    f\"\\n AUC: {test_auc.mean():.3} \\n MCC: {test_mcc.mean():.3} \\n \\\n",
    "Accuracy on test set: {test_accuracies.mean():.3} \\n Sensitivity: {test_tpr.mean():.3} \\n Specificity: {test_tnr.mean():.3}\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    "    fontweight=700,\n",
    "    fontsize=12.5\n",
    ")\n",
    "\n",
    "e = range(train_accuracies.shape[0])\n",
    "ax1.plot(e, train_accuracies)\n",
    "ax1.plot(e, valid_accuracies)\n",
    "ax1.plot(e, test_accuracies)\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_title(\"Accuracies evolution for 10 models over epochs\")\n",
    "ax1.set_ylim(.5, 1)\n",
    "\n",
    "ax2.plot(e, train_accuracies.mean(1))\n",
    "ax2.plot(e, valid_accuracies.mean(1))\n",
    "ax2.plot(e, test_accuracies.mean(1))\n",
    "ax2.set_title(\"Mean accuracies\")\n",
    "ax2.legend([\"Training\", \"Validation\", \"Test\"])\n",
    "ax2.set_ylim(.5, 1)\n",
    "\n",
    "ax3.plot(e, train_losses)\n",
    "ax3.plot(e, valid_losses)\n",
    "ax3.plot(e, test_losses)\n",
    "ax3.set_ylabel(\"Loss\")\n",
    "ax3.set_title(\"Learning curve of 10 models over epochs\")\n",
    "\n",
    "ax4.plot(e, train_losses.mean(1))\n",
    "ax4.plot(e, valid_losses.mean(1))\n",
    "ax4.plot(e, test_losses.mean(1))\n",
    "ax4.set_title(\"Mean learning curves\")\n",
    "ax4.legend([\"Training\", \"Validation\", \"Test\"])\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    wspace=.2,\n",
    "    hspace=.3\n",
    ")\n",
    "fig.savefig(f\"./{model_name}_results.png\", transparent=False, facecolor=\"white\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0f11ea36265c7a09575b72ab023a91955654eee7154dd519d25c1189c46f9fd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deeprank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
