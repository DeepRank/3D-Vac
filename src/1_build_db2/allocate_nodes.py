import pandas as pd
import argparse
import math
import subprocess
import re

arg_parser = argparse.ArgumentParser(
    description="This script is used to calculate the number of necessary nodes to call based on the total number of \
    cases in data/external/processed/to_model.csv file, generated by get_unmodelled_cases.py. This script is not made \
    to be run by itself, it is part of the 1_build_db2 pipeline and is executed by allocate_nodes.sh. \
    This script spawns the number of jobs required to model every unmodelled cases, with regards to \
    the number of cases being modeled by each core."
)
arg_parser.add_argument("--running-time", "-t",
    help="Number of hours spawned jobs will run. default 01.",
    default="01",
)
a = arg_parser.parse_args()

csv_path = "../../data/external/processed/to_model.csv"
df = pd.read_csv(csv_path)
tot_cases = len(df)


num_cores = 128
#total number of cases per hour for each node: (3600/(time for modeling a case for a core))*num_cores
#10 is an optimized factor representing 3600/(time for modeling a case for a core)
cases_per_hour_per_node = 10*num_cores # 1280 per hour per core. 
batch = cases_per_hour_per_node*int(a.running_time)
n_nodes = math.ceil(tot_cases/batch)

# additional hours are added to the running time to be sure every anchors is predicted
additional_hours = int(batch/cases_per_hour_per_node) # one hour is enough to predict all anchors from 1280 cases
sbatch_hours = str(int(a.running_time) + additional_hours).zfill(2) 
print("additional hours:", additional_hours)
print("total running time (in hours):", sbatch_hours)

modeling_job_stdout = subprocess.check_output([
    "sbatch",
    f"--nodes={n_nodes}",
    f"--time={sbatch_hours}:00:00",
    "modelling_job.sh",
    str(a.running_time), 
]).decode("ASCII")

jid = int(re.search(r"\d+", modeling_job_stdout).group())

# after the modelling job ended, run the cleaning job:
subprocess.run([
    "sbatch",
    f"--dependency=afterany:{jid}",
    "clean_outputs.sh"
])