import pandas as pd
import argparse
import math
import datetime
import subprocess
from copy import deepcopy
import re

arg_parser = argparse.ArgumentParser(
    description="This script is used to calculate the number of necessary nodes to call based on the total number of \
    cases in data/external/processed/to_model.csv file, generated by get_unmodelled_cases.py. This script is not made \
    to be run by itself, it is part of the 1_build_db2 pipeline and is executed by allocate_nodes.sh. \
    This script spawns the number of jobs required to model every unmodelled cases, with regards to \
    the number of cases being modeled by each core."
)
arg_parser.add_argument("--running-time", "-t",
    help="Number of hours spawned jobs will run. default 01.",
    default="01",
)
arg_parser.add_argument("--num-nodes", "-n",
    help = "Number of nodes to use. Default 0 (will use running-time instead).",
    default = "0",
)
arg_parser.add_argument("--input-csv", "-i",
    help="This argument allows to keep track of the db1 after modelling to check how many models are left."
)
arg_parser.add_argument("--mhc-class", "-m",
    help="MHC class of the cases",
    choices=['I','II'],
    required=True,
)
arg_parser.add_argument("--models-dir", "-p",
    help= "path of the models directory",
    default="/projects/0/einf2380/data/pMHCI/models/BA"
)

a = arg_parser.parse_args()

csv_path =  ('/').join(a.input_csv.split('/')[:-1]) + "/to_model.csv"


df = pd.read_csv(csv_path)
tot_cases = len(df)
running_time_hms = datetime.timedelta(hours=int(a.running_time))

NUM_CORES = 128
CASES_PER_HOUR_PER_CORE = 10

if a.num_nodes == "0":
    #no number of nodes given, compute nodes from running_time
    #total number of cases per hour for each node: (3600/(time for modeling a case for a core))*num_cores
    #10 is an optimized factor representing 3600/(time for modeling a case for a core)
    cases_per_hour_per_node = CASES_PER_HOUR_PER_CORE*NUM_CORES 
    batch = cases_per_hour_per_node*int(a.running_time)
    a.num_nodes = '{:0>2}'.format(math.ceil(tot_cases/batch))
else:
    # number of nodes given, compute running time per node
    cases_per_hour_per_node = CASES_PER_HOUR_PER_CORE*NUM_CORES
    running_time_frac  = tot_cases / (cases_per_hour_per_node * int(a.num_nodes))
    running_time_min = math.ceil(running_time_frac * 60)
    running_time_hms = datetime.timedelta(minutes=running_time_min)
    batch = math.ceil(cases_per_hour_per_node*running_time_frac)

# additional hours are added to the running time to be sure every anchors is predicted
# additional_hours = datetime.timedelta(hours=batch/cases_per_hour_per_node) # one hour is enough to predict all anchors from 1280 cases
sbatch_hours = str(running_time_hms).split('.')[0] # remove the microseconds

if (running_time_hms) < datetime.timedelta(minutes=15): # run at least 15 minutes, small batches tend to time out
    sbatch_hours = str(datetime.timedelta(minutes=15))

# sbatch_hours = str(int(a.running_time) + additional_hours).zfill(2) 
print("total running time (in hours):", str(sbatch_hours))

modelling_job_cmd_temp = [
    "sbatch",
    f"--time={sbatch_hours}",
    "--cpus-per-task={}",
    "modelling_job.sh",
    "--node-index={}",
    '--running-time', str(running_time_hms), 
    '--mhc-class', a.mhc_class,
    '--csv-path', csv_path,
    '--batch-size', str(batch)
]

# queue the jobs in serial based on the number of nodes needed
job_ids = []
for n in range(int(a.num_nodes)):
    # check if we can use less cores in the last batch
    n_cores = NUM_CORES
    if n == int(a.num_nodes)-1: # last one of the batches
        rest = tot_cases % batch
        assert tot_cases >= batch
        if not int(rest) == 0:
            n_cores = math.ceil(rest/CASES_PER_HOUR_PER_CORE) # compute cores based on batch rest
        elif batch < n_cores:
            n_cores = batch

    modelling_job_cmd = deepcopy(modelling_job_cmd_temp)
    # fill in missing parameters
    modelling_job_cmd[4] = modelling_job_cmd[4].format(n)
    modelling_job_cmd[2] = modelling_job_cmd[2].format(n_cores)

    print(f"running:\n {modelling_job_cmd}")

    modeling_job_stdout = subprocess.check_output(modelling_job_cmd).decode("ASCII")

    modelling_job_id = re.search(r"\d+", modeling_job_stdout).group()
    job_ids.append(modelling_job_id)

# after the modelling job ended, run the cleaning job:
clean_output_job_stdout = subprocess.check_output([
    "sbatch",
    f"--dependency=afterany:{','.join(job_ids)}",
    "clean_outputs.sh",
    "--models-dir", a.models_dir,
    "--mhc-class", a.mhc_class
]).decode("ASCII")

clean_output_job_id = int(re.search(r"\d+", clean_output_job_stdout).group())

subprocess.run([
    "sbatch",
    f"--dependency=afterok:{clean_output_job_id}",
    "get_unmodelled_cases.sh",
    "--csv-file", a.input_csv,
    "--models-dir", a.models_dir,
    "--parallel",
    "--archived", 
])
