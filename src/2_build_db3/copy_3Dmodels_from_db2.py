import argparse
import pandas as pd
import glob
import os
import math
import traceback
import subprocess
import tarfile
from joblib import Parallel, delayed 

arg_parser = argparse.ArgumentParser(
    description="""
    Copies target models from db2 into db2_selected_models folder to build db3 on top of them. The default behavior is to select 
    the top1 model generated by PANDORA (ranked in the molpdf.tsv file for each db2 case).
    """
)
arg_parser.add_argument("--structure-rank", "-r",
    help="""
    How many pdb structure to pick from each case. This argument corresponds to the n-th rank. For instance if -r 4,
    the number of structures selected from each case will be the 4 first structures in the molpdf.tsv file. For now,
    This whole pipeline is working only for 1 structure per db2 case. Therefore this argument shouldn't be tweaked, for now.
    Default 1.""",
    default=1,
    type=int,
)
arg_parser.add_argument("--csv-file", "-f",
    help="""
    Name of db1 in data/external/processed/. Default BA_pMHCI.csv.
    """,
    default="../../data/external/processed/BA_pMHCI.csv",
)
arg_parser.add_argument("--models-path", "-p",
    help = "glob.glob() string argument to generate a list of all models. A short tutorial on how to use glob.glob: \
    https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/\
     Default value: \
    /projects/0/einf2380/data/pMHCI/3D_models/BA/\*/\*",
    default = "/projects/0/einf2380/data/pMHCI/3D_models/BA/\*/\*"
)
arg_parser.add_argument("--mhc-class", "-m",
    help="""
    MHC class
    """,
    default="I",
    choices=["I", "II"],
)


def extract_member(case, member_name):
    try:
        with tarfile.open(f'{case}.tar', 'r') as tar:
            # need to rename tar members because writing to the original dir will not work otherwise
            for member in tar:
                member.name = os.path.basename(member.name)

            case_path = os.path.join(temp, os.path.basename(case))
            try:
                if not os.path.exists(case_path):
                    os.mkdir(case_path)
            except FileExistsError:
                print("folder already made")
            tar.extract(member_name, os.path.join(temp, os.path.basename(case)))
        # return the full path of tar member
        return os.path.join(temp, os.path.basename(case), member_name)
    except tarfile.ReadError as e:
        print(e)
        # remove complete directory so that get_unmodelled cases sees it as unmodelled
        print(f"(extract_member) Tar file is not valid, removing: {case}")
        # subprocess.run(f'rm -r {case}.tar', shell=True, check=True)
        return False
    except subprocess.CalledProcessError as e:
        print('In extract_member: ')
        print(e)
    except:
        traceback.print_exc()# if all goes well the tar file should exist

def retrieve_best_model(case):
    molpdf_path = extract_member(case, "molpdf_DOPE.tsv")
    try:
        molpdf_df = pd.read_csv(molpdf_path, sep="\t", header=None)
    except pd.errors.EmptyDataError:
        print(f"empty df: {molpdf_path}")
        return
    target_scores = molpdf_df.iloc[:,1].sort_values()[0:a.structure_rank]
    target_mask = [score in target_scores.tolist() for score in molpdf_df.iloc[:,1]]
    target_ids = molpdf_df[target_mask].iloc[:,0]
    targets = [f"{case}/{structure}" for structure in target_ids]
    return targets

def copy_best_model(case, pdb_names):
    # iterate of top n models (usually n=1)
    for pdb_name in pdb_names:
        # extract model from tar so it can be copied
        basename_pdb = os.path.basename(pdb_name)
        pdb_path = extract_member(case, basename_pdb)

        # building the output path
        case_path = "/".join(pdb_name.split("/")[-4:-1])
        pdb_file = pdb_name.split("/")[-1].split('.')
        pdb_file = ('.').join([pdb_file[0], pdb_file[2]])
        destination_dir = f"{db2_selected_models_path}/{case_path}/pdb"
        destination_file = f"{destination_dir}/{pdb_file}"

        if not os.path.isdir(destination_dir):
            try: # create remaining subfolders:
                os.makedirs(destination_dir)
            except:
                print('Something went wrong in creating', destination_dir)
        else:
            print(f"Directory {destination_dir} already exists")
        try: #Copy the pdb file to two files, one to be kept unchanged and one to be modified later
            subprocess.run(f'cp {pdb_path} {destination_file}', check=True, shell=True)
            subprocess.run(f'cp {pdb_path} {destination_file}.origin', check=True, shell=True)
            # now remove the temp files
            subprocess.run(f'rm -r {os.path.split(pdb_path)[0]}', check=True, shell=True)
        except Exception as e:
            print(f'The following error occurred: {e}')

def run(case_paths):
    for case in case_paths:
        try:
            targets = retrieve_best_model(case)
            copy_best_model(case, targets)
        except:
            print(traceback.format_exc())
            print(f'Copying best models of case: {case} not succesful')


a = arg_parser.parse_args()

db2_selected_models_path = f"/projects/0/einf2380/data/pMHC{a.mhc_class}/db2_selected_models_1"
# TMP dir to write temporary files to 
base_tmp = os.environ["TMPDIR"]
temp = os.path.join(base_tmp, "db3_copy_3Dmodels")
if not os.path.exists(temp):
    os.mkdir(temp)

# do a check if models dir is passed in the correct way
if "*" not in a.models_path and type(a.models_path)!=list:
    print("Expected a wild card path, please provide a path like this: mymodelsdir/\*/\*")
    raise SystemExit

# read the csv from db2 to find all case ids
csv_path = f"{a.csv_file}"
df = pd.read_csv(csv_path, header=0)

wildcard_path = a.models_path.replace('\\', '')
folders = glob.glob(wildcard_path)
folders = [folder for folder in folders if '.tar' in folder]
all_models = [case.split('.')[0] for case in folders]
# filter out the models that match in the original csv from db2
db2 = [folder for folder in all_models if "_".join(folder.split("/")[-1].split("_")[0:2]) in df["ID"].tolist()]

n_cores = int(os.getenv('SLURM_CPUS_ON_NODE'))

all_paths_lists = []
chunk = math.ceil(len(db2)/n_cores)
# cut the process into pieces to prevent spawning too many parallel processing
for i in range(0, len(db2), chunk):
    all_paths_lists.append(db2[i:min(i+chunk, len(db2))])
# let each inner list be handled by exactly one thread
Parallel(n_jobs = n_cores, verbose = 1)(delayed(run)(case) for case in all_paths_lists)

