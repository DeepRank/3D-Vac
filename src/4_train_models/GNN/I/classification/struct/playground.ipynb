{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from deeprankcore.Trainer import Trainer\n",
    "from deeprankcore.naive_gnn import NaiveNetwork\n",
    "from deeprankcore.DataSet import HDF5DataSet, save_hdf5_keys\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef)\n",
    "\n",
    "# set random seed!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input data\n",
    "protein_class = 'I'\n",
    "target_data = 'BA'\n",
    "resolution_data = 'residue' # either 'residue' or 'atomic'\n",
    "run_day_data = '17102022'\n",
    "# Target/s\n",
    "target_group = 'target_values'\n",
    "target_dataset = 'binary'\n",
    "task = 'classif'\n",
    "# Features\n",
    "node_features = [\n",
    "    \"res_type\",\n",
    "    \"res_charge\",\n",
    "    \"res_size\",\n",
    "    \"polarity\",\n",
    "    \"hb_donors\",\n",
    "    \"hb_acceptors\",\n",
    "    \"pssm\", \n",
    "    \"info_content\",\n",
    "    \"bsa\",\n",
    "    \"hse\",\n",
    "    \"sasa\",\n",
    "    \"res_depth\"]\n",
    "\n",
    "edge_features = [\n",
    "    \"same_chain\",\n",
    "    \"distance\",\n",
    "    \"covalent\",\n",
    "    \"electrostatic\",\n",
    "    \"vanderwaals\"]\n",
    "\n",
    "# Clusters\n",
    "cluster_dataset = 'cluster'\n",
    "train_clusters = [0, 1, 2, 3, 4, 7, 9]\n",
    "val_clusters = [5, 8]\n",
    "test_clusters = [6]\n",
    "# Trainer\n",
    "net = NaiveNetwork\n",
    "task = 'classif'\n",
    "batch_size = 16\n",
    "optimizer = torch.optim.Adam\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "epochs = 10\n",
    "save_model = 'best'\n",
    "# Paths\n",
    "project_folder = '/home/dbodor/git/DeepRank/3D-Vac/local_data/' # local resized df path\n",
    "# project_folder = '/projects/0/einf2380/'\n",
    "folder_data = f'{project_folder}data/pMHC{protein_class}/features_output_folder/GNN/{resolution_data}/{run_day_data}'\n",
    "input_data_path = folder_data + '/' + resolution_data + '.hdf5'\n",
    "# Experiment naming\n",
    "exp_name = 'rando'\n",
    "exp_date = True # bool\n",
    "exp_suffix = ''\n",
    "####################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folders and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs folder\n",
    "exp_basepath = './experiments/'\n",
    "exp_id = exp_name + '0'\n",
    "if os.path.exists(exp_basepath):\n",
    "    exp_list = [f for f in os.listdir(exp_basepath) if f.lower().startswith(exp_name.lower())]\n",
    "    if len(exp_list) > 0:\n",
    "        last_id = max([int(w[len(exp_name):].split('_')[0]) for w in exp_list])\n",
    "        print(last_id)\n",
    "        exp_id = exp_name + str(last_id + 1)\n",
    "exp_path = os.path.join(exp_basepath, exp_id)\n",
    "if exp_date:\n",
    "    today = datetime.now().strftime('%y%m%d')\n",
    "    exp_path += '_' + today\n",
    "if exp_suffix:\n",
    "    exp_path += '_' + exp_suffix\n",
    "os.makedirs(exp_path)\n",
    "\n",
    "data_path = os.path.join(exp_path, 'data')\n",
    "metrics_path = os.path.join(exp_path, 'metrics')\n",
    "img_path = os.path.join(exp_path, 'images')\n",
    "os.makedirs(data_path)\n",
    "os.makedirs(metrics_path)\n",
    "os.makedirs(img_path)\n",
    "# Loggers\n",
    "_log = logging.getLogger('')\n",
    "_log.setLevel(logging.INFO)\n",
    "\n",
    "fh = logging.FileHandler(os.path.join(exp_path, 'training.log'))\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "fh.setLevel(logging.INFO)\n",
    "sh.setLevel(logging.INFO)\n",
    "formatter_fh = logging.Formatter('[%(asctime)s] - %(name)s - %(message)s',\n",
    "                               datefmt='%a, %d %b %Y %H:%M:%S')\n",
    "fh.setFormatter(formatter_fh)\n",
    "\n",
    "_log.addHandler(fh)\n",
    "_log.addHandler(sh)\n",
    "####################\n",
    "\n",
    "_log.info(f'Created folder {exp_path}\\n')\n",
    "\n",
    "_log.info(\"training.py has started!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {}\n",
    "summary['entry'] = []\n",
    "summary['cluster'] = []\n",
    "summary['target'] = []\n",
    "summary['phase'] = []\n",
    "\n",
    "# '/Users/giuliacrocioni/remote_snellius/data/pMHCI/features_output_folder/GNN/residue/13072022/residue.hdf5'\n",
    "with h5py.File(input_data_path, 'r') as hdf5:\n",
    "\n",
    "    for mol in hdf5.keys():\n",
    "        cluster_value = float(hdf5[mol][target_group][cluster_dataset][()])\n",
    "        target_value = float(hdf5[mol][target_group][target_dataset][()])\n",
    "\n",
    "        summary['entry'].append(mol)\n",
    "        summary['cluster'].append(cluster_value)\n",
    "        summary['target'].append(target_value)\n",
    "\n",
    "        if cluster_value in train_clusters:\n",
    "            summary['phase'].append('train')\n",
    "        elif cluster_value in val_clusters:\n",
    "            summary['phase'].append('valid')\n",
    "        elif cluster_value in test_clusters:\n",
    "            summary['phase'].append('test')\n",
    "\n",
    "df_summ = pd.DataFrame(data=summary)\n",
    "\n",
    "df_summ.to_hdf(\n",
    "    os.path.join(metrics_path, 'summary_data.hdf5'),\n",
    "    key='summary',\n",
    "    mode='w')\n",
    "\n",
    "df_train = df_summ[df_summ.phase == 'train']\n",
    "df_valid = df_summ[df_summ.phase == 'valid']\n",
    "df_test = df_summ[df_summ.phase == 'test']\n",
    "\n",
    "_log.info(f'Data statistics:\\n')\n",
    "_log.info(f'Total samples: {len(df_summ)}')\n",
    "_log.info(f'Training set: {len(df_train)} samples, {round(100*len(df_train)/len(df_summ))}%')\n",
    "_log.info(f'\\t- Class 0: {len(df_train[df_train.target == 0])} samples, {round(100*len(df_train[df_train.target == 0])/len(df_train))}%')\n",
    "_log.info(f'\\t- Class 1: {len(df_train[df_train.target == 1])} samples, {round(100*len(df_train[df_train.target == 1])/len(df_train))}%')\n",
    "_log.info(f'Validation set: {len(df_valid)} samples, {round(100*len(df_valid)/len(df_summ))}%')\n",
    "_log.info(f'\\t- Class 0: {len(df_valid[df_valid.target == 0])} samples, {round(100*len(df_valid[df_valid.target == 0])/len(df_valid))}%')\n",
    "_log.info(f'\\t- Class 1: {len(df_valid[df_valid.target == 1])} samples, {round(100*len(df_valid[df_valid.target == 1])/len(df_valid))}%')\n",
    "_log.info(f'Testing set: {len(df_test)} samples, {round(100*len(df_test)/len(df_summ))}%')\n",
    "_log.info(f'\\t- Class 0: {len(df_test[df_test.target == 0])} samples, {round(100*len(df_test[df_test.target == 0])/len(df_test))}%')\n",
    "_log.info(f'\\t- Class 1: {len(df_test[df_test.target == 1])} samples, {round(100*len(df_test[df_test.target == 1])/len(df_test))}%')\n",
    "\n",
    "for cl in sorted(df_summ.cluster.unique(), reverse=True):\n",
    "    if len(df_summ[df_summ.cluster == cl]):\n",
    "        _log.info(f'\\t\\tCluster {int(cl)}: {len(df_summ[df_summ.cluster == cl])} samples, {round(100*len(df_summ[df_summ.cluster == cl])/len(df_summ))}%')\n",
    "        _log.info(f'\\t\\t\\t- Class 0: {len(df_summ[(df_summ.cluster == cl) & (df_summ.target == 0)])} samples, {round(100*len(df_summ[(df_summ.cluster == cl) & (df_summ.target == 0)])/len(df_summ[df_summ.cluster == cl]))}%')\n",
    "        _log.info(f'\\t\\t\\t- Class 1: {len(df_summ[(df_summ.cluster == cl) & (df_summ.target == 1)])} samples, {round(100*len(df_summ[(df_summ.cluster == cl) & (df_summ.target == 1)])/len(df_summ[df_summ.cluster == cl]))}%')\n",
    "    else:\n",
    "        _log.info(f'Cluster {int(cl)} not present!')\n",
    "\n",
    "save_hdf5_keys(input_data_path, df_summ[df_summ.phase == 'train'].entry.to_list(), os.path.join(data_path, 'train.hdf5'), hardcopy = True)\n",
    "save_hdf5_keys(input_data_path, df_summ[df_summ.phase == 'valid'].entry.to_list(), os.path.join(data_path, 'valid.hdf5'), hardcopy = True)\n",
    "save_hdf5_keys(input_data_path, df_summ[df_summ.phase == 'test'].entry.to_list(), os.path.join(data_path, 'test.hdf5'), hardcopy = True)\n",
    "####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(f'HDF5DataSet loading...\\n')\n",
    "# to change: pass in only list of keys\n",
    "dataset_train = HDF5DataSet(\n",
    "    hdf5_path = [\n",
    "        os.path.join(data_path, 'train.hdf5')],\n",
    "    target = target_dataset,\n",
    "    task = task,\n",
    "    node_feature = node_features,\n",
    "    edge_feature = edge_features\n",
    ")\n",
    "dataset_val = HDF5DataSet(\n",
    "    hdf5_path = [\n",
    "        os.path.join(data_path, 'valid.hdf5')],\n",
    "    target = target_dataset,\n",
    "    task = task,\n",
    "    node_feature = node_features,\n",
    "    edge_feature = edge_features\n",
    ")\n",
    "dataset_test = HDF5DataSet(\n",
    "    hdf5_path = [\n",
    "        os.path.join(data_path, 'test.hdf5')],\n",
    "    target = target_dataset,\n",
    "    task = task,\n",
    "    node_feature = node_features,\n",
    "    edge_feature = edge_features\n",
    ")\n",
    "_log.info(f'Len df train: {len(dataset_train)}')\n",
    "_log.info(f'Len df valid: {len(dataset_val)}')\n",
    "_log.info(f'Len df test: {len(dataset_test)}')\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_log.info(f'Instantiating Trainer...\\n')\n",
    "\n",
    "trainer = Trainer(\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    dataset_test,\n",
    "    net,\n",
    "    batch_size = batch_size,\n",
    "    output_dir = metrics_path\n",
    ")\n",
    "trainer.configure_optimizers(optimizer, lr, weight_decay)\n",
    "trainer.train(nepoch = epochs, validate = True, save_model = save_model, model_path = os.path.join(exp_path, 'model.tar'))\n",
    "trainer.test()\n",
    "\n",
    "epoch = trainer.epoch_saved_model\n",
    "_log.info(f\"Model saved at epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_json = {}\n",
    "\n",
    "## store input settings\n",
    "exp_json['exp_id'] = exp_id\n",
    "exp_json['exp_fullname'] = exp_path.split('/')[-1]\n",
    "exp_json['exp_path'] = exp_path\n",
    "exp_json['input_data_path'] = input_data_path\n",
    "exp_json['protein_class'] = protein_class\n",
    "exp_json['target_data'] = target_data\n",
    "exp_json['resolution'] = resolution_data\n",
    "exp_json['target_data'] = target_data\n",
    "exp_json['task'] = task\n",
    "exp_json['node_features'] = [node_features]\n",
    "exp_json['edge_features'] = [edge_features]\n",
    "exp_json['net'] = str(net)\n",
    "exp_json['optimizer'] = str(optimizer)\n",
    "exp_json['max_epochs'] = epochs\n",
    "exp_json['batch_size'] = batch_size\n",
    "exp_json['lr'] = lr\n",
    "exp_json['weight_decay'] = weight_decay\n",
    "exp_json['save_state'] = save_model\n",
    "exp_json['train_clusters'] = [train_clusters]\n",
    "exp_json['val_clusters'] = [val_clusters]\n",
    "exp_json['test_clusters'] = [test_clusters]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load output and retrieve metrics\n",
    "exp_json['epoch'] = epoch\n",
    "exp_json['last_epoch'] = epochs # adjust if/when we add an early stop\n",
    "\n",
    "metrics_df = pd.read_hdf(os.path.join(metrics_path, 'metrics.hdf5'), 'metrics')\n",
    "d = {'thr': [], 'precision': [], 'recall': [], 'accuracy': [], 'f1': [], 'mcc': [], 'auc': [], 'aucpr': [], 'phase': []}\n",
    "thr_df = pd.DataFrame(data=d)\n",
    "df_epoch = metrics_df[(metrics_df.epoch == epoch) | ((metrics_df.epoch == 0) & (metrics_df.phase == 'testing'))]\n",
    "\n",
    "for phase in ['training', 'validation', 'testing']:\n",
    "    df_epoch_phase = df_epoch[(df_epoch.phase == phase)]\n",
    "    y_true = df_epoch_phase.target\n",
    "    y_score = np.array(df_epoch_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    thrs = np.linspace(0,1,100)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    mcc = []\n",
    "    \n",
    "    for thr in thrs:\n",
    "        y_pred = (y_score > thr)*1\n",
    "        precision.append(precision_score(y_true, y_pred, zero_division=0))\n",
    "        recall.append(recall_score(y_true, y_pred, zero_division=0))\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "        mcc.append(matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    fpr_roc, tpr_roc, thr_roc = roc_curve(y_true, y_score)\n",
    "    auc_score = auc(fpr_roc, tpr_roc)\n",
    "    aucpr = average_precision_score(y_true, y_score)\n",
    "\n",
    "    phase_df = pd.DataFrame({'thr': thrs, 'precision': precision, 'recall': recall, 'accuracy': accuracy, 'f1': f1, 'mcc': mcc, 'auc': auc_score, 'aucpr': aucpr, 'phase': phase})\n",
    "    thr_df = pd.concat([thr_df, phase_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max mcc of test set\n",
    "test_df = thr_df.loc[thr_df.phase == 'testing']\n",
    "test_mcc_idxmax = test_df.mcc.idxmax()\n",
    "if thr_df.loc[test_mcc_idxmax].mcc > 0:\n",
    "    sel_thr = thr_df.loc[test_mcc_idxmax].thr\n",
    "# use max mcc of all data if max of test set is 0 (usually only on small local test experiments)\n",
    "else:\n",
    "    mcc_idxmax = thr_df.mcc.idxmax()\n",
    "    sel_thr = thr_df.loc[mcc_idxmax].thr\n",
    "    _log.info(\"WARNING: Maximum mcc of test set is 0. Instead, maximum mcc of all data will be used for determining optimal threshold.\\n\")\n",
    "\n",
    "test_mcc_idxmax, sel_thr, mcc_idxmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store output\n",
    "exp_json['training_loss'] = metrics_df[(metrics_df.epoch == epoch) & (metrics_df.phase == 'training')].loss.mean()\n",
    "exp_json['validation_loss'] = metrics_df[(metrics_df.epoch == epoch) & (metrics_df.phase == 'validation')].loss.mean()\n",
    "exp_json['testing_loss'] = metrics_df[(metrics_df.epoch == epoch) & (metrics_df.phase == 'testing')].loss.mean()\n",
    "for score in ['mcc', 'auc', 'aucpr', 'f1', 'accuracy', 'precision', 'recall']:\n",
    "    for phase in ['training', 'validation', 'testing']:\n",
    "        exp_json[f'{phase}_{score}'] = round(float(thr_df[(thr_df.thr == sel_thr) & (thr_df.phase == phase)][score]), 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to excel file\n",
    "Note that this gives a column headers will not match stored data if new headers are added or removed between experiments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(exp_basepath + '_experiments_log.xlsx')\n",
    "file_exists = filename.is_file()\n",
    "\n",
    "exp_df = pd.DataFrame(exp_json, index=[0])\n",
    "\n",
    "with pd.ExcelWriter(\n",
    "    filename,\n",
    "    engine=\"openpyxl\",\n",
    "    mode=\"a\" if file_exists else \"w\",\n",
    "    if_sheet_exists='overlay' if file_exists else None,\n",
    ") as writer:\n",
    "\n",
    "    if file_exists:\n",
    "        _log.info(\"Updating metadata in experiments_log.xlsx ...\\n\")\n",
    "        old_df = pd.read_excel(filename)\n",
    "        exp_df = pd.concat([exp_df, old_df]) # newest experiment on top\n",
    "    else:\n",
    "        _log.info(\"Creating metadata in experiments_log.xlsx ...\\n\")\n",
    "    exp_df.to_excel(writer, sheet_name='All', index=False, header=True)\n",
    "\n",
    "_log.info(\"Saved! End of the training script\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('deeprank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1ae6c54777279c6e2baf96ef286b4134d72358d4655dc5f46a2b8e21aee4a2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
