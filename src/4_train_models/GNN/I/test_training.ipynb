{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import cProfile, pstats, io\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef)\n",
    "import torch\n",
    "from deeprankcore.trainer import Trainer\n",
    "from deeprankcore.utils.exporters import HDF5OutputExporter\n",
    "from deeprankcore.dataset import GraphDataset\n",
    "from deeprankcore.neuralnets.gnn.naive_gnn import NaiveNetwork\n",
    "#from pmhc_gnn import PMHCI_Network01\n",
    "\n",
    "# initialize\n",
    "starttime = datetime.now()\n",
    "torch.manual_seed(22) #11 22 33 44 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### To fill\n",
    "# Input data\n",
    "# run_day_data = '230329' # 692 data points (local folder)\n",
    "run_day_data = '230329' # 100k data points (proj folder)\n",
    "# Paths\n",
    "protein_class = 'I'\n",
    "target_data = 'BA'\n",
    "resolution_data = 'residue' # either 'residue' or 'atomic'\n",
    "project_folder = '/home/ccrocion/snellius_data_sample' # local resized df path\n",
    "#project_folder = '/projects/0/einf2380'\n",
    "folder_data = f'{project_folder}/data/pMHC{protein_class}/features_output_folder/GNN/{resolution_data}/{run_day_data}'\n",
    "input_data_path = glob.glob(os.path.join(folder_data, '*.hdf5'))\n",
    "# Experiment naming\n",
    "exp_name = 'Batch Size 128_1'\n",
    "exp_date = True # bool\n",
    "exp_suffix = ''\n",
    "# Target/s\n",
    "target_group = 'target_values'\n",
    "target_dataset = 'binary'\n",
    "task = 'classif'\n",
    "standardize = True\n",
    "# Clusters\n",
    "# If cluster_dataset is None, sets are randomly splitted\n",
    "cluster_dataset = None # 'cl_allele'# None # 'allele_type'\n",
    "cluster_dataset_type = 'string' # None # 'string'\n",
    "# train_clusters = [0, 1, 2, 3, 4, 7, 9]\n",
    "# val_clusters = [5, 8]\n",
    "test_clusters = ['C']\n",
    "# Dataset\n",
    "# node_features = [\n",
    "#     'bsa', 'hb_acceptors', 'hb_donors',\n",
    "#     'hse', 'info_content', 'irc_negative_negative',\n",
    "#     'irc_negative_positive', 'irc_nonpolar_negative', 'irc_nonpolar_nonpolar',\n",
    "#     'irc_nonpolar_polar', 'irc_nonpolar_positive', 'irc_polar_negative',\n",
    "#     'irc_polar_polar', 'irc_polar_positive', 'irc_positive_positive',\n",
    "#     'irc_total', 'polarity',\n",
    "#     'res_charge', 'res_depth', 'res_mass',\n",
    "#     'res_pI', 'res_size', 'res_type', 'sasa']\n",
    "# node_features = \"all\"\n",
    "node_features = \"all\"\n",
    "# edge_features = [\n",
    "#     \"covalent\", \"distance\", \"same_chain\", \"electrostatic\", \"vanderwaals\"]\n",
    "# edge_features = \"all\"\n",
    "edge_features = \"all\"\n",
    "\n",
    "# standardize & transform Dictionary\n",
    "feat_trans_dict={'bsa':{'transform':lambda t:np.log(t+1),'standardize':True},\n",
    "               'res_depth':{'transform':lambda t:np.log(t+1),'standardize':True},\n",
    "               'info_content':{'transform':lambda t:np.log(t+1),'standardize':True},\n",
    "               'sasa':{'transform':lambda t:np.sqrt(t),'standardize':True},\n",
    "               'electrostatic':{'transform':lambda t:np.cbrt(t),'standardize':True},\n",
    "               'vanderwaals':{'transform':lambda t:np.cbrt(t),'standardize':True},\n",
    "               'res_size':{'transform':None,'standardize':True},\n",
    "               'res_charge':{'transform':None,'standardize':True},\n",
    "               'hb_donors':{'transform':None,'standardize':True},\n",
    "               'hb_acceptors':{'transform':None,'standardize':True},\n",
    "               'hse':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_negative':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_nonpolar':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_polar':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_positive':{'transform':None,'standardize':True},\n",
    "               'irc_polar_polar':{'transform':None,'standardize':True},\n",
    "               'irc_polar_positive':{'transform':None,'standardize':True},\n",
    "               'irc_total':{'transform':None,'standardize':True},\n",
    "               'irc_negative_positive':{'transform':None,'standardize':True},\n",
    "               'irc_positive_positive':{'transform':None,'standardize':True},\n",
    "               'irc_polar_negative':{'transform':None,'standardize':True},\n",
    "               'irc_negative_negative':{'transform':None,'standardize':True},\n",
    "               'res_mass':{'transform':None,'standardize':True},\n",
    "               'res_pI':{'transform':None,'standardize':True},\n",
    "               'distance':{'transform':None,'standardize':True},\n",
    "               'pssm':{'transform':None,'standardize':True}}\n",
    "\n",
    "feat_notrans_dict={'bsa':{'transform':None,'standardize':True},\n",
    "               'res_depth':{'transform':None,'standardize':True},\n",
    "               'info_content':{'transform':None,'standardize':True},\n",
    "               'sasa':{'transform':None,'standardize':True},\n",
    "               'electrostatic':{'transform':None,'standardize':True},\n",
    "               'vanderwaals':{'transform':None,'standardize':True},\n",
    "               'res_size':{'transform':None,'standardize':True},\n",
    "               'res_charge':{'transform':None,'standardize':True},\n",
    "               'hb_donors':{'transform':None,'standardize':True},\n",
    "               'hb_acceptors':{'transform':None,'standardize':True},\n",
    "               'hse':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_negative':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_nonpolar':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_polar':{'transform':None,'standardize':True},\n",
    "               'irc_nonpolar_positive':{'transform':None,'standardize':True},\n",
    "               'irc_polar_polar':{'transform':None,'standardize':True},\n",
    "               'irc_polar_positive':{'transform':None,'standardize':True},\n",
    "               'irc_total':{'transform':None,'standardize':True},\n",
    "               'irc_negative_positive':{'transform':None,'standardize':True},\n",
    "               'irc_positive_positive':{'transform':None,'standardize':True},\n",
    "               'irc_polar_negative':{'transform':None,'standardize':True},\n",
    "               'irc_negative_negative':{'transform':None,'standardize':True},\n",
    "               'res_mass':{'transform':None,'standardize':True},\n",
    "               'res_pI':{'transform':None,'standardize':True},\n",
    "               'distance':{'transform':None,'standardize':True},\n",
    "               'pssm':{'transform':None,'standardize':True}}\n",
    "\n",
    "feat_notrans_nostd_dict={'bsa':{'transform':None,'standardize':False},\n",
    "               'res_depth':{'transform':None,'standardize':False},\n",
    "               'info_content':{'transform':None,'standardize':False},\n",
    "               'sasa':{'transform':None,'standardize':False},\n",
    "               'electrostatic':{'transform':None,'standardize':False},\n",
    "               'vanderwaals':{'transform':None,'standardize':False},\n",
    "               'res_size':{'transform':None,'standardize':False},\n",
    "               'res_charge':{'transform':None,'standardize':False},\n",
    "               'hb_donors':{'transform':None,'standardize':False},\n",
    "               'hb_acceptors':{'transform':None,'standardize':False},\n",
    "               'hse':{'transform':None,'standardize':False},\n",
    "               'irc_nonpolar_negative':{'transform':None,'standardize':False},\n",
    "               'irc_nonpolar_nonpolar':{'transform':None,'standardize':False},\n",
    "               'irc_nonpolar_polar':{'transform':None,'standardize':False},\n",
    "               'irc_nonpolar_positive':{'transform':None,'standardize':False},\n",
    "               'irc_polar_polar':{'transform':None,'standardize':False},\n",
    "               'irc_polar_positive':{'transform':None,'standardize':False},\n",
    "               'irc_total':{'transform':None,'standardize':False},\n",
    "               'irc_negative_positive':{'transform':None,'standardize':False},\n",
    "               'irc_positive_positive':{'transform':None,'standardize':False},\n",
    "               'irc_polar_negative':{'transform':None,'standardize':False},\n",
    "               'irc_negative_negative':{'transform':None,'standardize':False},\n",
    "               'res_mass':{'transform':None,'standardize':False},\n",
    "               'res_pI':{'transform':None,'standardize':False},\n",
    "               'distance':{'transform':None,'standardize':False},\n",
    "               'pssm':{'transform':None,'standardize':False}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "net = NaiveNetwork\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.Adam\n",
    "lr = 1e-3\n",
    "weight_decay = 0\n",
    "epochs = 40\n",
    "save_model = 'best'\n",
    "class_weights = True # weighted loss function\n",
    "cuda = False\n",
    "ngpu = 0\n",
    "num_workers = 16\n",
    "train_profiling = False\n",
    "check_integrity = True\n",
    "# early stopping\n",
    "earlystop_patience = 15\n",
    "earlystop_maxgap = 0.06\n",
    "min_epoch = 20\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Folders and logger\n",
    "# Outputs folder\n",
    "exp_basepath = './experiments/'\n",
    "exp_id = exp_name + '0'\n",
    "if os.path.exists(exp_basepath):\n",
    "    exp_list = [f for f in os.listdir(exp_basepath) if f.lower().startswith(exp_name.lower())]\n",
    "    if len(exp_list) > 0:\n",
    "        last_id = max([int(w[len(exp_name):].split('_')[0]) for w in exp_list])\n",
    "        exp_id = exp_name + str(last_id + 1)\n",
    "exp_path = os.path.join(exp_basepath, exp_id)\n",
    "if exp_date:\n",
    "    today = starttime.strftime('%y%m%d')\n",
    "    exp_path += '_' + today\n",
    "if exp_suffix:\n",
    "    exp_path += '_' + exp_suffix\n",
    "os.makedirs(exp_path)\n",
    "\n",
    "data_path = os.path.join(exp_path, 'data')\n",
    "output_path = os.path.join(exp_path, 'output')\n",
    "img_path = os.path.join(exp_path, 'images')\n",
    "os.makedirs(data_path)\n",
    "os.makedirs(output_path)\n",
    "os.makedirs(img_path)\n",
    "# Loggers\n",
    "_log = logging.getLogger('')\n",
    "_log.setLevel(logging.INFO)\n",
    "\n",
    "fh = logging.FileHandler(os.path.join(exp_path, 'training.log'))\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "fh.setLevel(logging.INFO)\n",
    "sh.setLevel(logging.INFO)\n",
    "formatter_fh = logging.Formatter('[%(asctime)s] - %(name)s - %(message)s',\n",
    "                               datefmt='%a, %d %b %Y %H:%M:%S')\n",
    "fh.setFormatter(formatter_fh)\n",
    "\n",
    "_log.addHandler(fh)\n",
    "_log.addHandler(sh)\n",
    "####################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _log.info(f'Created folder {exp_path}\\n')\n",
    "\n",
    "    _log.info(\"training.py has started!\\n\")\n",
    "\n",
    "    #################### Data summary\n",
    "    summary = {}\n",
    "    summary['entry'] = []\n",
    "    summary['target'] = []\n",
    "\n",
    "    if cluster_dataset is not None:\n",
    "        summary['cluster'] = []\n",
    "\n",
    "    for fname in input_data_path:\n",
    "        try:\n",
    "            with h5py.File(fname, 'r') as hdf5:\n",
    "                for mol in hdf5.keys():\n",
    "                    target_value = float(hdf5[mol][target_group][target_dataset][()])\n",
    "                    summary['entry'].append(mol)\n",
    "                    summary['target'].append(target_value)\n",
    "\n",
    "                    if cluster_dataset is not None:\n",
    "                        if cluster_dataset_type == 'string':\n",
    "                            cluster_value = hdf5[mol][target_group][cluster_dataset].asstr()[()]\n",
    "                        else:\n",
    "                            cluster_value = float(hdf5[mol][target_group][cluster_dataset][()])\n",
    "\n",
    "                        summary['cluster'].append(cluster_value)\n",
    "\n",
    "        except Exception as e:\n",
    "            _log.error(e)\n",
    "            _log.info(f'Error in opening {fname}, please check the file.')\n",
    "    \n",
    "    df_summ = pd.DataFrame(data=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_dataset is None:\n",
    "    # random split\n",
    "    df_train, df_test = train_test_split(df_summ, test_size=0.1, stratify=df_summ.target, random_state=42)\n",
    "    df_train, df_valid = train_test_split(df_train, test_size=0.2, stratify=df_train.target, random_state=42)\n",
    "else:\n",
    "    # use cluster for test, random split for train and valid\n",
    "    df_test = df_summ[df_summ.cluster.isin(test_clusters)]\n",
    "    df_train = df_summ[~df_summ.cluster.isin(test_clusters)]\n",
    "    df_train, df_valid = train_test_split(df_train, test_size=0.2, stratify=df_train.target, random_state=42)\n",
    "\n",
    "df_summ['phase'] = ['test' if entry in df_test.entry.values else 'valid' if entry in df_valid.entry.values else 'train' for entry in df_summ.entry]\n",
    "\n",
    "df_summ.to_hdf(\n",
    "    os.path.join(output_path, 'summary_data.hdf5'),\n",
    "    key='summary',\n",
    "    mode='w')\n",
    "\n",
    "_log.info(f'Data statistics:\\n')\n",
    "_log.info(f'Total samples: {len(df_summ)}\\n')\n",
    "if cluster_dataset is not None:\n",
    "    _log.info(f'Clustering on Dataset: {cluster_dataset}.\\n')\n",
    "_log.info(f'Training set: {len(df_train)} samples, {round(100*len(df_train)/len(df_summ))}%')\n",
    "_log.info(f'\\t- Class 0: {len(df_train[df_train.target == 0])} samples, {round(100*len(df_train[df_train.target == 0])/len(df_train))}%')\n",
    "_log.info(f'\\t- Class 1: {len(df_train[df_train.target == 1])} samples, {round(100*len(df_train[df_train.target == 1])/len(df_train))}%')\n",
    "if cluster_dataset is not None:\n",
    "    _log.info(f'Clusters present: {df_train.cluster.unique()}\\n')\n",
    "_log.info(f'Validation set: {len(df_valid)} samples, {round(100*len(df_valid)/len(df_summ))}%')\n",
    "_log.info(f'\\t- Class 0: {len(df_valid[df_valid.target == 0])} samples, {round(100*len(df_valid[df_valid.target == 0])/len(df_valid))}%')\n",
    "_log.info(f'\\t- Class 1: {len(df_valid[df_valid.target == 1])} samples, {round(100*len(df_valid[df_valid.target == 1])/len(df_valid))}%')\n",
    "if cluster_dataset is not None:\n",
    "    _log.info(f'Clusters present: {df_valid.cluster.unique()}\\n')\n",
    "_log.info(f'Testing set: {len(df_test)} samples, {round(100*len(df_test)/len(df_summ))}%')\n",
    "_log.info(f'\\t- Class 0: {len(df_test[df_test.target == 0])} samples, {round(100*len(df_test[df_test.target == 0])/len(df_test))}%')\n",
    "_log.info(f'\\t- Class 1: {len(df_test[df_test.target == 1])} samples, {round(100*len(df_test[df_test.target == 1])/len(df_test))}%')\n",
    "if cluster_dataset is not None:\n",
    "    _log.info(f'Clusters present: {df_test.cluster.unique()}\\n')\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(f'HDF5DataSet loading...\\n')\n",
    "dataset_train = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_train.entry),\n",
    "    target = target_dataset,\n",
    "    task = task,\n",
    "    node_features = node_features,\n",
    "    edge_features = edge_features,\n",
    "    #standardize = standardize,\n",
    "    check_integrity = check_integrity,\n",
    "    features_transform = feat_notrans_dict\n",
    ")\n",
    "_log.info(f'Dataset_Train Setted\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprank_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
