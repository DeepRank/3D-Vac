{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from CNN.models import MlpRegBaseline\n",
    "from CNN.datasets import Class_Seq_Dataset\n",
    "from CNN.datasets import load_class_seq_data\n",
    "import matplotlib.pylab as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful functions:\n",
    "\n",
    "def load_model(model_path, plot=False):\n",
    "    cp = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "    xval_data = cp[\"models_data\"]\n",
    "    a = cp[\"arguments\"]\n",
    "\n",
    "    train_losses = torch.tensor([data[\"train_losses\"] for data in xval_data])\n",
    "    validation_losses = torch.tensor([data[\"validation_losses\"] for data in xval_data])\n",
    "    train_losses = torch.permute(train_losses, (1,0))\n",
    "    validation_losses = torch.permute(validation_losses, (1,0))\n",
    "    if plot:\n",
    "        #plot mean validation and loss\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.plot(range(a.epochs), train_losses.mean(dim=1))\n",
    "        plt.plot(range(a.epochs), validation_losses.mean(dim=1))\n",
    "        plt.title(\"Valid\")\n",
    "        plt.xlabel(\"Iterations over dataset\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(f\"Mean learning curve of 10 models from \\n {os.path.basename(model_path).replace('_', ' ')}\")\n",
    "        plt.legend([\"Training\", \"Validation\"])\n",
    "        plt.show()\n",
    "\n",
    "        #plot the actual validation and loss\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.plot(range(a.epochs), train_losses)\n",
    "        plt.plot(range(a.epochs), validation_losses)\n",
    "        plt.title(\"Validation and losses\")\n",
    "        plt.xlabel(\"Iterations over dataset\")\n",
    "        plt.show()\n",
    "    return xval_data, a\n",
    "\n",
    "def get_rates(xval_data, a):\n",
    "    csv_peptides, csv_ba_values, groups = load_class_seq_data(\"../../data/external/processed/BA_pMHCI.csv\", a.threshold)\n",
    "    dataset = Class_Seq_Dataset(csv_peptides, csv_ba_values, a.encoder, torch.device(\"cpu\"))\n",
    "    accuracies = []\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    tnr = []\n",
    "    fnr = []\n",
    "    nr = []\n",
    "    pr = []\n",
    "    for data in xval_data:\n",
    "        # load the model and data\n",
    "        input_dimensions = (20*9, 40*9)[a.encoder==\"mixed\"]\n",
    "        model = MlpRegBaseline(neurons_per_layer=a.neurons, input=input_dimensions, outputs=2) \n",
    "        model.load_state_dict(data[\"model\"])\n",
    "        model.eval()\n",
    "\n",
    "        test_X = dataset.peptides[data[\"test_indices\"]]\n",
    "        test_label = dataset.labels[data[\"test_indices\"]]\n",
    "\n",
    "        ### MAKE THE PREDICTION ON THE TEST DATASET ###\n",
    "        with torch.no_grad():\n",
    "            test_pred_label = model(test_X).max(1)[1]\n",
    "\n",
    "        ### CALCULATE SENSITIVITY, SPECIFICITY AND OTHER METRICS ###\n",
    "        # Calculate the confusion tensor (AKA matching matrix) by dividing predictions\n",
    "        # with true values.\n",
    "        # To assess performances, first calculate absolute values and adjust it to \n",
    "        # the total number of expected postives, negatives:\n",
    "\n",
    "        # calculate absolute values:\n",
    "        confusion = test_pred_label/test_label # absolute values for metrics\n",
    "        tot = test_label.shape[0] # total number of prediction\n",
    "        pos = float((test_label == 1.).sum()) # total number of positives (truth 1)\n",
    "        neg = float((test_label == 0.).sum()) # total number of negatives (truth 0)\n",
    "\n",
    "        # calculate rates:\n",
    "        tpr.append( float((confusion == 1.).sum()/pos) ) # true positive rate = prediction 1/truth 1\n",
    "        fnr.append( float((confusion == 0.).sum()/pos)) # false negative rate = predicted 0/truth 1\n",
    "        tnr.append( float(torch.isnan(confusion).sum()/neg) ) # true negative rate = predicted 0/truth 0\n",
    "        fpr.append( float((confusion == float(\"inf\")).sum()/neg) ) # false positive rate = predicted 1/truth 0\n",
    "\n",
    "        # calculate raw accuracy:\n",
    "        accuracy = float((test_pred_label == test_label).sum()/test_label.shape[0])\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # to keep track of truths in test sets\n",
    "        nr.append( neg/tot ) \n",
    "        pr.append( pos/tot )\n",
    "\n",
    "    ### STORE METRICS ###\n",
    "    accuracies = torch.tensor(accuracies)\n",
    "    tpr = torch.tensor(tpr)\n",
    "    fnr = torch.tensor(fnr)\n",
    "    tnr = torch.tensor(tnr)\n",
    "    fpr = torch.tensor(fpr)\n",
    "    nr = torch.tensor(nr)\n",
    "    pr = torch.tensor(pr)\n",
    "    return tpr, fnr, tnr, fpr, nr, pr, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"/home/daqop/Desktop/M2/Internship/python/trained_models/\"\n",
    "# models_path = \"/home/daqop/mountpoint_snellius/3D-Vac/src/5_train_models/CNN/I/reg/seq/trained_models/\"\n",
    "# models_path = \"/home/daqop/train_branch_3D_Vac/src/5_train_models/CNN/I/classification/seq/trained_models/\"\n",
    "model_names = {\n",
    "    \"shuffled\": \"mlp_classification_mean_mixed_encoder_1000_neurons_200_epochs_shuffled_64_batch_size.pt\",\n",
    "    \"clustered\": \"mlp_classification_mean_mixed_encoder_1000_neurons_200_epochs_clustered_64_batch_size.pt\",\n",
    "}\n",
    "shuffled_xval, shuffled_a = load_model(models_path + model_names[\"shuffled\"])\n",
    "clustered_xval, clustered_a = load_model(models_path + model_names[\"clustered\"])\n",
    "s_tpr, s_fnr, s_tnr, s_fpr, s_nr, s_pr, s_accuracy = get_rates(shuffled_xval, shuffled_a)\n",
    "c_tpr, c_fnr, c_tnr, c_fpr, c_nr, c_pr, c_accuracy = get_rates(clustered_xval, clustered_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled PPV: tensor(0.8228)\n",
      "Clustered PPV: tensor(0.7612)\n",
      "Shuffled Accuracy: tensor(0.8370)\n",
      "Clustered Accuracy: tensor(0.7813)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shuffled PPV:\", s_tpr.mean())\n",
    "print(\"Clustered PPV:\", c_tpr.mean())\n",
    "print(\"Shuffled Accuracy:\", s_accuracy.mean())\n",
    "print(\"Clustered Accuracy:\", c_accuracy.mean())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0f11ea36265c7a09575b72ab023a91955654eee7154dd519d25c1189c46f9fd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deeprank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
