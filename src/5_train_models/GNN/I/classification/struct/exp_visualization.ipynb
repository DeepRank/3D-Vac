{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from deeprankcore.Trainer import Trainer\n",
    "from deeprankcore.ginet import GINet\n",
    "from deeprankcore.DataSet import HDF5DataSet, save_hdf5_keys\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### To fill\n",
    "exp_id = 'exp7'\n",
    "####################\n",
    "exp_df = pd.read_excel('./experiments/_experiments_log.xlsx', index_col='exp_id')\n",
    "exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fullname = exp_df.loc[exp_id].exp_fullname\n",
    "exp_path = os.path.join('experiments', exp_fullname)\n",
    "metrics_path = os.path.join(exp_path, 'metrics')\n",
    "img_path = os.path.join(exp_path, 'images')\n",
    "\n",
    "df_summ = pd.read_hdf(os.path.join(metrics_path, 'summary_data.hdf5'), 'summary')\n",
    "metrics = pd.read_hdf(os.path.join(metrics_path, 'metrics.hdf5'), 'metrics')\n",
    "print(df_summ.shape)\n",
    "print(metrics.shape)\n",
    "\n",
    "df = pd.merge(metrics, df_summ[['cluster', 'entry']], on=\"entry\", how=\"outer\")\n",
    "df.sort_values(by=['epoch'], inplace = True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(df.cluster.notna().all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss vs epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    df[(df.phase =='training') | (df.phase =='validation')],\n",
    "    x='epoch',\n",
    "    y='loss',\n",
    "    color='phase',\n",
    "    markers=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch #',\n",
    "    yaxis_title='Loss',\n",
    "    width=800, height=500,\n",
    "    title='Loss vs epochs',\n",
    "    title_x=0.5,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=50,\n",
    "        pad=4),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        #y=0.99\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "        )\n",
    ")\n",
    "epoch = exp_df.loc[exp_id].epoch\n",
    "fig.add_vline(x=epoch, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(img_path, 'loss_epoch.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification metrics (for best/saved epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df[(df.epoch == epoch) | ((df.epoch == 0) & (df.phase == 'testing'))]\n",
    "print(df_plot.shape)\n",
    "y_true = df_plot.target\n",
    "y_score = np.array(df_plot.output.values.tolist())[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only target distributions per target and cluster (data exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = df_plot.cluster.unique()\n",
    "clusters.sort()\n",
    "clusters = list(clusters)\n",
    "df_plot['target_str'] = df_plot['target'].astype(str)\n",
    "# The histogram of scores compared to true labels\n",
    "fig = px.histogram(\n",
    "    df_plot,\n",
    "    x='target_str',\n",
    "    color=df_plot.cluster,\n",
    "    facet_row='phase',\n",
    "    category_orders={'phase': [\n",
    "        'training',\n",
    "        \"validation\",\n",
    "        \"testing\"],\n",
    "        'cluster': clusters}\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=600, height=600,\n",
    "    showlegend=True,\n",
    "    title='Target',\n",
    "    title_x=0.5,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=50,\n",
    "        pad=4\n",
    "    )\n",
    ")\n",
    "# fig.update_yaxes(range=[0, 2700], constrain='domain')\n",
    "fig.update_layout(bargap=0.30,bargroupgap=0.0)\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(img_path, 'target_only.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target and score distributions per target and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = df_plot.cluster.unique()\n",
    "clusters.sort()\n",
    "clusters = list(clusters)\n",
    "# The histogram of scores compared to true labels\n",
    "fig = px.histogram(\n",
    "    df_plot,\n",
    "    x=y_score,\n",
    "    color=df_plot.cluster,\n",
    "    nbins=20,\n",
    "    facet_col='target',\n",
    "    facet_row='phase',\n",
    "    labels=dict(color='True Labels', x='Score'),\n",
    "    category_orders={'phase': [\n",
    "        'training',\n",
    "        \"validation\",\n",
    "        \"testing\"],\n",
    "        'cluster': clusters}\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=900, height=600,\n",
    "    showlegend=True,\n",
    "    title='Target and scores',\n",
    "    title_x=0.5,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=50,\n",
    "        pad=4\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y = 1.05,\n",
    "        xanchor=\"left\",\n",
    "        x=0.87\n",
    "        )\n",
    ")\n",
    "fig.update_xaxes(range=[0, 1], constrain='domain')\n",
    "# fig.update_yaxes(range=[0, 750], constrain='domain')\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(img_path, 'target_scores.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles = ['ROC Curves (AUC)', 'PR Curves (AUCPR)'], horizontal_spacing = 0.05)\n",
    "colors = [\"darkcyan\", \"coral\", \"cornflowerblue\"]\n",
    "\n",
    "for idx, set in enumerate(['training', 'validation', 'testing']):\n",
    "    df_plot_phase = df_plot[(df_plot.phase == set)]\n",
    "    y_true = df_plot_phase.target\n",
    "    y_score = np.array(df_plot_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    fpr_roc, tpr_roc, thr_roc = roc_curve(y_true, y_score)\n",
    "    pr_pr, rec_pr, thr_pr = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    name_roc = f'AUC={auc(fpr_roc, tpr_roc):.4f}'\n",
    "    name_pr = f'AUCPR={average_precision_score(y_true, y_score):.4f}'\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr_roc,\n",
    "        y=tpr_roc,\n",
    "        name=name_roc,\n",
    "        mode='markers+lines',\n",
    "        legendgroup=set,\n",
    "        legendgrouptitle_text=f\"{set}\",\n",
    "        marker_color = colors[idx]),\n",
    "        row=1,\n",
    "        col=1)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=rec_pr,\n",
    "        y=pr_pr,\n",
    "        name=name_pr,\n",
    "        mode='markers+lines',\n",
    "        legendgroup=set,\n",
    "        marker_color = colors[idx]),\n",
    "        row=1,\n",
    "        col=2)\n",
    "\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1,\n",
    "    row = 1, col = 1\n",
    ")\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=1, y1=0,\n",
    "    row = 1, col = 2 \n",
    ")\n",
    "fig.update_layout(\n",
    "    width=900, height=400,\n",
    "    margin=go.layout.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=50,\n",
    "        t=50,\n",
    "        pad=4\n",
    "    ),\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=1.05,\n",
    "        xanchor=\"left\",\n",
    "        x=0.95\n",
    "        ))\n",
    "fig.update_xaxes(title_text=\"FPR\", constrain='domain', scaleratio = 1, row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"TPR (Recall)\", constrain='domain', scaleanchor = \"x\", scaleratio = 1, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Recall\", constrain='domain', scaleanchor = \"y\", scaleratio = 1, row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Precision\", constrain='domain', scaleratio = 1, row=1, col=2)\n",
    "fig.write_html(os.path.join(img_path, 'auc_aucpr.html'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics vs thresholds curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'thr': [], 'precision': [], 'recall': [], 'accuracy': [], 'f1': [], 'mcc': [], 'auc': [], 'aucpr': [], 'phase': []}\n",
    "thr_df = pd.DataFrame(data=d)\n",
    "\n",
    "for idx, set in enumerate(['training', 'validation', 'testing']):\n",
    "    df_plot_phase = df_plot[(df_plot.phase == set)]\n",
    "    y_true = df_plot_phase.target\n",
    "    y_score = np.array(df_plot_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    thrs = np.linspace(0,1,100)\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "    mcc = []\n",
    "    \n",
    "    for thr in thrs:\n",
    "        y_pred = (y_score > thr)*1\n",
    "        precision.append(precision_score(y_true, y_pred))\n",
    "        recall.append(recall_score(y_true, y_pred))\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        f1.append(f1_score(y_true, y_pred))\n",
    "        mcc.append(matthews_corrcoef(y_true, y_pred))\n",
    "    \n",
    "    auc_score = auc(fpr_roc, tpr_roc)\n",
    "    aucpr = average_precision_score(y_true, y_score)\n",
    "\n",
    "    phase_df = pd.DataFrame({'thr': thrs, 'precision': precision, 'recall': recall, 'accuracy': accuracy, 'f1': f1, 'mcc': mcc, 'auc': auc_score, 'aucpr': aucpr, 'phase': set})\n",
    "    thr_df = pd.concat([thr_df, phase_df], ignore_index=True)\n",
    "\n",
    "# find max mcc of test set\n",
    "test_df = thr_df.loc[thr_df.phase == 'testing']\n",
    "test_mcc_idxmax = test_df.mcc.idxmax()\n",
    "if thr_df.loc[test_mcc_idxmax].mcc > 0:\n",
    "    sel_thr = thr_df.loc[test_mcc_idxmax].thr\n",
    "# use max mcc of all data if max of test set is 0 (usually only on small local test experiments)\n",
    "else:\n",
    "    mcc_idxmax = thr_df.mcc.idxmax()\n",
    "    sel_thr = thr_df.loc[mcc_idxmax].thr\n",
    "    print(\"WARNING: Maximum mcc of test set is 0. Instead, maximum mcc of all data will be used for determining optimal threshold.\\n\")\n",
    "\n",
    "fig_thresh = px.line(\n",
    "    thr_df,\n",
    "    x='thr',\n",
    "    y=[\n",
    "        'precision',\n",
    "        'recall',\n",
    "        'accuracy',\n",
    "        'f1',\n",
    "        'mcc'\n",
    "    ],\n",
    "    facet_col='phase',\n",
    "    category_orders={'phase': [\n",
    "        'training',\n",
    "        \"validation\",\n",
    "        \"testing\"]},\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "fig_thresh.add_vline(x=sel_thr, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "fig_thresh.update_layout(\n",
    "    title='Metrics vs thresholds',\n",
    "    title_x=0.5)\n",
    "fig_thresh.update_yaxes(range=[-0.2, 1.2], scaleanchor=\"x\", scaleratio=1, constrain='domain')\n",
    "fig_thresh.update_xaxes(range=[0, 1], scaleratio = 1, constrain='domain')\n",
    "fig_thresh.write_html(os.path.join(img_path, 'thresholds_metrics.html'))\n",
    "fig_thresh.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('deeprank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1ae6c54777279c6e2baf96ef286b4134d72358d4655dc5f46a2b8e21aee4a2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
